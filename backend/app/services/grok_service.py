"""
LLM Service - GPT-5 via GitHub Models API
Falls back to template-based responses if API unavailable
"""

import logging
from typing import Optional
from openai import AsyncOpenAI
from app.core.config import settings

logger = logging.getLogger(__name__)


class LLMService:
    """GPT-5 powered plant response generator via GitHub Models API"""

    def __init__(self):
        self.client = None
        self.model = settings.GITHUB_MODELS_MODEL
        self._initialize_client()

    def _initialize_client(self):
        """Initialize OpenAI client for GitHub Models API"""
        if settings.GITHUB_TOKEN:
            try:
                self.client = AsyncOpenAI(
                    base_url=settings.GITHUB_MODELS_BASE_URL,
                    api_key=settings.GITHUB_TOKEN,
                )
                logger.info(
                    f"âœ… GPT-5 via GitHub Models initialized (model: {self.model})"
                )
            except Exception as e:
                logger.error(f"âŒ Failed to initialize GitHub Models client: {e}")
                self.client = None
        else:
            logger.warning("âš ï¸ GITHUB_TOKEN not set - using template-based responses")

    async def generate_response(
        self, prompt: str, context: Optional[str] = None
    ) -> str:
        """Generate response using GPT-5 or fallback to template"""
        if self.client:
            try:
                return await self._generate_gpt5_response(prompt, context)
            except Exception as e:
                logger.error(f"âŒ GPT-5 API error: {e}")
                return self._generate_template_response(prompt, context)
        else:
            return self._generate_template_response(prompt, context)

    async def _generate_gpt5_response(
        self, prompt: str, context: Optional[str] = None
    ) -> str:
        """Generate response using GPT-5 via GitHub Models API"""
        system_prompt = """Sen bir botanik uzmanÄ±sÄ±n. KullanÄ±cÄ±lara bitki tanÄ±mlama ve bilgi saÄŸlama konusunda yardÄ±mcÄ± oluyorsun.
YanÄ±tlarÄ±nÄ± her zaman TÃ¼rkÃ§e olarak ver. Bilimsel ve yararlÄ± bilgiler sun.
Emojiler kullanarak yanÄ±tlarÄ±nÄ± daha okunabilir yap."""

        messages = [
            {"role": "system", "content": system_prompt},
        ]

        if context:
            messages.append(
                {
                    "role": "user",
                    "content": f"Bitki bilgileri:\n{context}\n\nKullanÄ±cÄ± sorusu: {prompt}",
                }
            )
        else:
            messages.append({"role": "user", "content": prompt})

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
        )

        return response.choices[0].message.content

    def _generate_template_response(
        self, prompt: str, context: Optional[str] = None
    ) -> str:
        """Fallback: Generate formatted plant response from context"""
        if not context:
            return "Bitki analizi yapÄ±ldÄ± ancak eÅŸleÅŸen sonuÃ§ bulunamadÄ±. LÃ¼tfen daha net bir gÃ¶rsel ile tekrar deneyin."

        # Parse context to extract plant info
        response_parts = ["ðŸŒ¿ **GÃ¶rsel Analizi TamamlandÄ±!**\n"]

        # Add context directly - it's already formatted
        response_parts.append("**Bulunan Bitkiler:**")
        response_parts.append(context)
        response_parts.append("")

        # Add helpful info based on query type
        query_lower = prompt.lower()

        if any(word in query_lower for word in ["bakÄ±m", "sulama", "yetiÅŸtir", "care"]):
            response_parts.append("**ðŸ’¡ BakÄ±m Ã–nerileri:**")
            response_parts.append("- Bitkinin tÃ¼rÃ¼ne gÃ¶re sulama ihtiyacÄ± deÄŸiÅŸir")
            response_parts.append("- DolaylÄ± gÃ¼neÅŸ Ä±ÅŸÄ±ÄŸÄ± Ã§oÄŸu bitki iÃ§in idealdir")
            response_parts.append("- TopraÄŸÄ±n Ã¼st kÄ±smÄ± kuruduÄŸunda sulayÄ±n")

        elif any(
            word in query_lower for word in ["zehir", "tehlike", "toxic", "poison"]
        ):
            response_parts.append("**âš ï¸ UyarÄ±:**")
            response_parts.append(
                "- BazÄ± bitkiler evcil hayvanlar iÃ§in zararlÄ± olabilir"
            )
            response_parts.append("- DetaylÄ± bilgi iÃ§in uzman gÃ¶rÃ¼ÅŸÃ¼ alÄ±n")

        else:
            response_parts.append("**ðŸ“ Not:**")
            response_parts.append(
                "- YukarÄ±daki bilgiler Kaggle PlantCLEF, PlantNet ve USDA veritabanlarÄ±ndan alÄ±nmÄ±ÅŸtÄ±r"
            )
            response_parts.append("- Kesin tanÄ±mlama iÃ§in uzman gÃ¶rÃ¼ÅŸÃ¼ Ã¶nerilir")

        return "\n".join(response_parts)

    async def generate_rag_response(
        self, query: str, context: str, plants: list = None
    ) -> str:
        """RAG response with plant context"""
        return await self.generate_response(query, context)


# Global instance
grok_service = LLMService()
