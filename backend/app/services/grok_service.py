"""
LLM Service - Multi-Provider with Automatic Fallback Chain
Priority: GPT-5 (GitHub) â†’ Google AI Studio (Gemini) â†’ OpenRouter â†’ Template
"""

import logging
from typing import Optional, List, Tuple
from openai import AsyncOpenAI
from app.core.config import settings

logger = logging.getLogger(__name__)


class LLMService:
    """Multi-provider LLM service with automatic fallback chain"""

    def __init__(self):
        self.providers: List[Tuple[str, Optional[AsyncOpenAI], str]] = []
        self._initialize_providers()

    def _initialize_providers(self):
        """Initialize all available LLM providers in priority order"""
        self.providers = []

        # 1. GPT-5 via GitHub Models (Primary)
        if settings.GITHUB_TOKEN:
            try:
                client = AsyncOpenAI(
                    base_url=settings.GITHUB_MODELS_BASE_URL,
                    api_key=settings.GITHUB_TOKEN,
                )
                self.providers.append(
                    ("GPT-5 (GitHub)", client, settings.GITHUB_MODELS_MODEL)
                )
                logger.info(
                    f"âœ… Provider 1: GPT-5 via GitHub Models ({settings.GITHUB_MODELS_MODEL})"
                )
            except Exception as e:
                logger.warning(f"âš ï¸ GPT-5 init failed: {e}")

        # 2. Google AI Studio (Gemini) - First Fallback
        if settings.GOOGLE_AI_STUDIO_API_KEY:
            try:
                client = AsyncOpenAI(
                    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
                    api_key=settings.GOOGLE_AI_STUDIO_API_KEY,
                )
                self.providers.append(
                    ("Google AI (Gemini)", client, settings.GOOGLE_AI_STUDIO_MODEL)
                )
                logger.info(
                    f"âœ… Provider 2: Google AI Studio ({settings.GOOGLE_AI_STUDIO_MODEL})"
                )
            except Exception as e:
                logger.warning(f"âš ï¸ Google AI init failed: {e}")

        # 3. OpenRouter - Second Fallback
        if settings.OPENROUTER_API_KEY:
            try:
                client = AsyncOpenAI(
                    base_url=settings.OPENROUTER_BASE_URL,
                    api_key=settings.OPENROUTER_API_KEY,
                )
                self.providers.append(("OpenRouter", client, settings.OPENROUTER_MODEL))
                logger.info(f"âœ… Provider 3: OpenRouter ({settings.OPENROUTER_MODEL})")
            except Exception as e:
                logger.warning(f"âš ï¸ OpenRouter init failed: {e}")

        if not self.providers:
            logger.warning(
                "âš ï¸ No LLM providers configured - using template-based responses only"
            )
        else:
            logger.info(
                f"ðŸ”— LLM Fallback chain: {' â†’ '.join([p[0] for p in self.providers])} â†’ Template"
            )

    async def generate_response(
        self, prompt: str, context: Optional[str] = None
    ) -> str:
        """Generate response with automatic fallback through provider chain"""

        # Try each provider in order
        for provider_name, client, model in self.providers:
            try:
                logger.info(f"ðŸ”„ Trying {provider_name}...")
                response = await self._call_provider(client, model, prompt, context)
                logger.info(f"âœ… {provider_name} succeeded")
                return response
            except Exception as e:
                logger.warning(f"âš ï¸ {provider_name} failed: {e}")
                continue

        # All providers failed - use template
        logger.info("ðŸ“ All LLM providers failed - using template response")
        return self._generate_template_response(prompt, context)

    async def _call_provider(
        self,
        client: AsyncOpenAI,
        model: str,
        prompt: str,
        context: Optional[str] = None,
    ) -> str:
        """Call a specific LLM provider"""
        system_prompt = """Sen bir botanik uzmanÄ±sÄ±n. KullanÄ±cÄ±lara bitki tanÄ±mlama ve bilgi saÄŸlama konusunda yardÄ±mcÄ± oluyorsun.
YanÄ±tlarÄ±nÄ± her zaman TÃ¼rkÃ§e olarak ver. Bilimsel ve yararlÄ± bilgiler sun.
Emojiler kullanarak yanÄ±tlarÄ±nÄ± daha okunabilir yap."""

        messages = [
            {"role": "system", "content": system_prompt},
        ]

        if context:
            messages.append(
                {
                    "role": "user",
                    "content": f"Bitki bilgileri:\n{context}\n\nKullanÄ±cÄ± sorusu: {prompt}",
                }
            )
        else:
            messages.append({"role": "user", "content": prompt})

        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            timeout=settings.LLM_API_TIMEOUT,
        )

        return response.choices[0].message.content

    def _generate_template_response(
        self, prompt: str, context: Optional[str] = None
    ) -> str:
        """Last resort: Generate formatted plant response from context"""
        if not context:
            return "Bitki analizi yapÄ±ldÄ± ancak eÅŸleÅŸen sonuÃ§ bulunamadÄ±. LÃ¼tfen daha net bir gÃ¶rsel ile tekrar deneyin."

        # Parse context to extract plant info
        response_parts = ["ðŸŒ¿ **GÃ¶rsel Analizi TamamlandÄ±!**\n"]

        # Add context directly - it's already formatted
        response_parts.append("**Bulunan Bitkiler:**")
        response_parts.append(context)
        response_parts.append("")

        # Add helpful info based on query type
        query_lower = prompt.lower()

        if any(word in query_lower for word in ["bakÄ±m", "sulama", "yetiÅŸtir", "care"]):
            response_parts.append("**ðŸ’¡ BakÄ±m Ã–nerileri:**")
            response_parts.append("- Bitkinin tÃ¼rÃ¼ne gÃ¶re sulama ihtiyacÄ± deÄŸiÅŸir")
            response_parts.append("- DolaylÄ± gÃ¼neÅŸ Ä±ÅŸÄ±ÄŸÄ± Ã§oÄŸu bitki iÃ§in idealdir")
            response_parts.append("- TopraÄŸÄ±n Ã¼st kÄ±smÄ± kuruduÄŸunda sulayÄ±n")

        elif any(
            word in query_lower for word in ["zehir", "tehlike", "toxic", "poison"]
        ):
            response_parts.append("**âš ï¸ UyarÄ±:**")
            response_parts.append(
                "- BazÄ± bitkiler evcil hayvanlar iÃ§in zararlÄ± olabilir"
            )
            response_parts.append("- DetaylÄ± bilgi iÃ§in uzman gÃ¶rÃ¼ÅŸÃ¼ alÄ±n")

        else:
            response_parts.append("**ðŸ“ Not:**")
            response_parts.append(
                "- YukarÄ±daki bilgiler Kaggle PlantCLEF, PlantNet ve USDA veritabanlarÄ±ndan alÄ±nmÄ±ÅŸtÄ±r"
            )
            response_parts.append("- Kesin tanÄ±mlama iÃ§in uzman gÃ¶rÃ¼ÅŸÃ¼ Ã¶nerilir")

        return "\n".join(response_parts)

    async def generate_rag_response(
        self, query: str, context: str, plants: list = None
    ) -> str:
        """RAG response with plant context"""
        return await self.generate_response(query, context)


# Global instance
grok_service = LLMService()
